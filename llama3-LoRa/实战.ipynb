{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa5d73a-f8f5-40c3-a4c6-d2350ac3d25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8d161-ae6e-487f-9f4d-66725f84df89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bitsandbytes：专为量化设计的库，重点在于减少大语言模型（尤其是在GPU上）的内存占用。\n",
    "# peft：用于将LoRA适配器集成到大语言模型（LLMs）中。\n",
    "# trl：该库包含一个SFT（监督微调）类，用于辅助微调模型。\n",
    "# accelerate和xformers：这些库用于提高模型的推理速度，从而优化其性能。\n",
    "# wandb：该工具作为一个监控平台，用于跟踪和观察训练过程。\n",
    "# datasets：与Hugging Face一起使用，该库便于加载数据集。\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import os, wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42bb5d9-31d1-4719-bec6-7e61bf1bd9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe1bb15-91c6-4d8e-b24c-005ae706972a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1957a81-b928-4aa2-9139-6a476c9f339e",
   "metadata": {},
   "source": [
    "## 1. 加载模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f91d603-5177-45ac-86ae-182d158319b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Meta-Llama-3-8B\"\n",
    "\n",
    "dataset_name = \"scooterman/guanaco-llama3-1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ad9fb8-0fdd-4069-a1a0-e2fe510c2bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练模型和tokenizer\n",
    "\n",
    "# 量化配置\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # 模型将以4位量化格式加载\n",
    "    bnb_4bit_quant_type = \"nf4\", # 指定4位量化的类型为 nf4 \n",
    "    bnb_4bit_compute_dtype = torch.float16, # 计算数据类型 \n",
    "    bnb_4bit_use_double_quant = False, # 表示不使用双重量化\n",
    ")\n",
    "\n",
    "# 模型加载\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = {\"\": 0} # 将模型加载到设备0（通常是第一个GPU）\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model) \n",
    "\n",
    "# tokenizer 加载\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True # 在生成序列时会自动添加结束标记\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321be706-7129-4eda-9206-691aebb8ba13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d7af1-be7c-423f-96ce-f85c91d66d51",
   "metadata": {},
   "source": [
    "## 2.wandb配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7063b7-555e-49ca-8eb0-31e4b37b9fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 监控需要在WandB官网注册账号\n",
    "\n",
    "wandb.login(key=\"自己的key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5c9b6-473a-4430-b933-6d8c750cf0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=\"finetune llama-3-8B\",\n",
    "    job_type = \"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be707db-4d63-4b9b-9742-423d4a345f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算训练参数量\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"训练参数量 : {trainable_params} || 总的参数量 : {all_param} || 训练参数量占比%: {100 * (trainable_params / all_param):.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe0889-626b-491e-b9f5-3040cc5ab266",
   "metadata": {},
   "source": [
    "## 3. LoRA与训练超参配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2140b-94a7-4494-a007-78ecbe7e54af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16, # 小技巧：把α值设置成rank值的两倍\n",
    "    # scaling = alpha / r # LoRA 权重的值越大，影响就越大。\n",
    "    # weight += (lora_B @ lora_A) * scaling\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    # [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"]\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111d14b-70b4-49b2-9668-c8ec54584758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练超参\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"/root/autodl-tmp/output\",\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 2, # 梯度累积步数为2，即每2步更新一次梯度。有助于在显存有限的情况下使用较大的有效批次大小。\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    save_steps = 100, # 每100步保存一次模型 \n",
    "    logging_steps = 30,\n",
    "    learning_rate = 2e-4,\n",
    "    weight_decay = 0.001, # 权重衰减系数，用于L2正则化，帮助防止过拟合。\n",
    "    fp16 = False,\n",
    "    bf16 = False,\n",
    "    max_grad_norm = 0.3, # 最大梯度范数，用于梯度裁剪，防止梯度爆炸。\n",
    "    max_steps = -1, # 最大训练步数为-1，表示没有限制。\n",
    "    warmup_ratio = 0.3, # 预热阶段的比例。在训练开始时，学习率会逐渐升高，预热比例为0.3表示前30%的训练步骤用于预热。\n",
    "    group_by_length = True, # 按序列长度分组，以提高训练效率。\n",
    "    lr_scheduler_type = \"linear\", # 表示使用线性学习率调度。\n",
    "    report_to = \"wandb\", # tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254d9d5-5cca-4b03-b16b-9c6c81cc8c8c",
   "metadata": {},
   "source": [
    "## 4. 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda6ecb-182a-4bfc-bc61-58f16f6f2455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SFT超参\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    peft_config = peft_config,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    args = training_arguments,\n",
    "    packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f768663-e041-4cb6-b9c8-cc675d64f91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2d593-e13f-42f9-8e80-9e09caadebaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 计算可训练参数量\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c4c98-29bb-4c24-a719-3530798088c7",
   "metadata": {},
   "source": [
    "## 5. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e145e-9445-4b6a-827e-4db15cdaaba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存微调模型\n",
    "\n",
    "trainer.model.save_pretrained(\"lora_model\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "model.config.use_cache = True\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de1c28-ab48-42b8-98e6-b7fd76cb8cc0",
   "metadata": {},
   "source": [
    "## 6. 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f7857-d793-484e-ba21-2177263a2b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base模型测试\n",
    "\n",
    "def stream(user_input):\n",
    "    device = \"cuda:0\"\n",
    "    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{E_INST}\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4be79-5ce9-4bdd-b081-2ec689ca2faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stream(\"Tell me something about the Great Wall.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8b61b-b0a1-4aea-b7d6-3199940b7afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import os, wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7534a26-c389-40d7-a596-a0c598ff87e4",
   "metadata": {},
   "source": [
    "## 7. 模型合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322f2f7-5917-4fbf-a530-e6838ea8242f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 预训练模型\n",
    "model_name = \"/root/autodl-tmp/model/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6117311-6eb6-4fca-91f6-6a4506fb0325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 合并 base model 与 lora model\n",
    "# https://huggingface.co/docs/trl/main/en/use_model#use-adapters-peft\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.float16,\n",
    "    device_map= {\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cbce1-6c68-4624-a964-1235a3693222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = PeftModel.from_pretrained(base_model, \"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68696e4a-d578-4dd3-b505-481f20afbf0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型合并\n",
    "\n",
    "merged_model = new_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d282d-a367-4e4b-a644-4b75fb7bcd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cecd9c-77c7-49cc-8b61-947180b55dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = \"Tell me something about the Great Wall.\"\n",
    "device = \"cuda:0\"\n",
    "system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{E_INST}\"\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "_ = merged_model.generate(**inputs, streamer=streamer, max_new_tokens=128, num_return_sequences=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
